\section{Discussion}
\label{sec:discussion}

In addition to perform POS tagging, we could try to group words that are similar to each other by encode each word to numerical vectors representation. This can be achieved using Word2Vec, a state-of-art tool that used to produce word embedding. Word2Vec is a group of models, Specifically, two-layer neural networks that are trained to reconstruct linguistic contexts of words. It takes a corpus as input and produces a vector space, with each unique words in the corpus assigned with a corresponding vectors.
Word2Vec was created by a group of researchers at Google led by Tomas Mikolov. Some advantages of Word2Vec over other rival tool such as WordNet is that it can better capture syntactic and semantic information and it can achieve a lower false positive rate than ESA (Explicit Semantic Analysis)  Word2Vec also offer two types of model to produce representation of word: Continuous bag-of-word (CBOW) or continuous skip-gram. In CBOW model, the model predicts a word from a range of surrounding words. On the other hand, a skip-gram model uses the current word to predict the surrounding windows of context words. This model is able to achieve this by assigning more weight to the neighboring words than more distant words.
This technique will be useful in that we will be able to group device with its capabilities and states, and this will have a more predicting power than a standard POS tagging approach.
We were unable to perform this techniques due to a limited number of data to train the model, but can be used for future work.
